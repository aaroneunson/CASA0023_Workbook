[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CASA0023_Workbook",
    "section": "",
    "text": "Introduction\nIntro content…"
  },
  {
    "objectID": "Week1.html",
    "href": "Week1.html",
    "title": "1  Week 1 - Getting Started With Remote Sensing",
    "section": "",
    "text": "trying to fix issues"
  },
  {
    "objectID": "Week3.html",
    "href": "Week3.html",
    "title": "3  Corrections and Enahancements",
    "section": "",
    "text": "Week 3 content …\nOutputs:\n\n\n\nRatio Enhancement - NVDI Greater Than 0.2; Merged Tile\n\n\n\n\n\nTexture Enhancement - Sample Tile\n\n\n\n\n\nTexture Enhancement - Merged Area"
  },
  {
    "objectID": "Week6.html",
    "href": "Week6.html",
    "title": "5  Week 6 - Google Earth Engine I",
    "section": "",
    "text": "Week 6 content…."
  },
  {
    "objectID": "Week4.html",
    "href": "Week4.html",
    "title": "4  Week 4 - Policy",
    "section": "",
    "text": "Week 4 content…\nNeed to decide on a policy!!"
  },
  {
    "objectID": "Week2.html",
    "href": "Week2.html",
    "title": "2  Using Xaringnan",
    "section": "",
    "text": "##Landsat 5 Profile\n\n\n\n\n\n\n\n\n\nHere is a presentation about the Landsat 5 satellite!"
  },
  {
    "objectID": "Week4.html#summary-summary-of-the-policy-and-city-you-have-selected.",
    "href": "Week4.html#summary-summary-of-the-policy-and-city-you-have-selected.",
    "title": "4  Policy",
    "section": "4.1 Summary: summary of the policy and city you have selected.",
    "text": "4.1 Summary: summary of the policy and city you have selected.\n\n4.1.1 Flood Risk in Jakarta, Indonesia\nJakarta faces a significant risk of flooding due to a combination of geographical, environmental, and human factors:\n\nGeographical Location: Jakarta is situated on the northwest coast of Java, at the mouth of the Ciliwung River on Jakarta Bay, which leads to the Java Sea. This coastal and low-lying geography makes it inherently vulnerable to flooding from the sea, especially during high tide events.\nLand Subsidence: Excessive groundwater extraction has caused Jakarta to sink at an alarming rate, with some areas experiencing subsidence rates of more than 10 cm per year. This subsidence exacerbates the risk of flooding, as the city becomes increasingly lower than sea level.\nUrbanization and Loss of Green Spaces: Rapid urban development has led to the loss of green spaces and water-absorbent areas, increasing runoff and reducing the natural absorption of rainwater. This situation is worsened by the fact that many developments occur without proper consideration for their environmental impact, contributing to the city’s vulnerability to floods.\nClimate Change: The effects of climate change, including rising sea levels and more intense rainfall events, heighten the risk of both coastal and pluvial flooding in Jakarta.\nInadequate Infrastructure: Jakarta’s drainage and flood management infrastructure are often inadequate to handle the volume of water from heavy rains and high tides, partly due to the rapid pace of urban development and challenges in governance and maintenance.\nRiver Overflow: The city is crossed by 13 rivers, and the overflow from these rivers during heavy rains is a significant cause of flooding in many parts of Jakarta.\n\nThese factors are interlinked, creating a complex challenge for flood risk management in Jakarta. Addressing these issues requires a multifaceted approach that includes improving infrastructure, regulating groundwater extraction, enhancing urban planning, and mitigating the effects of climate change.\nhttps://eastasiaforum.org/2021/07/13/better-flood-management-can-save-jakarta/ https://www.mdpi.com/2071-1050/10/8/2934\n\n\n4.1.2 Jakarta’s Current Strategy\nJakarta’s strategy for managing its significant flood risk has evolved to include both infrastructural developments and nature-based solutions (NbS), addressing the city’s unique challenges such as extreme urbanization, land subsidence, and the impacts of climate change.\nThe Jakarta Coastal Defense Strategy (JCDS) and the subsequent National Capital Integrated Coastal Development Masterplan (NCICD), colloquially known as the “Giant Sea Wall” project, stand out as major infrastructural components of Jakarta’s flood management efforts. Initiated with Dutch collaboration, these projects aim to protect the city from sea-level rise and coastal flooding by constructing massive sea walls and creating retention areas to manage excess water. This strategy is being complemented by efforts to regulate river flows and expand flood reservoirs, as well as the clearing of waterways to restore their capacity.\nhttps://www.mdpi.com/2071-1050/10/8/2934 https://floodlist.com/asia/plans-reduce-jakarta-flooding\nIn addition to these heavy engineering solutions, Jakarta is incorporating NbS into its flood management strategy. These solutions leverage natural processes to mitigate flood risks, such as enhancing green open spaces that can absorb rainwater, implementing bioswales and permeable pavements to manage runoff, and creating urban green infrastructures like green roofs and rain gardens. The NbS approach not only addresses flooding but also tackles other urban challenges like heat islands and water pollution, making it a multifaceted solution to urban environmental issues.\nhttps://wri-indonesia.org/en/insights/reasons-jakartas-frequent-flooding-and-how-nature-based-solutions-nbs-can-help-reduce-risk\nDespite the ambitious plans and potential benefits of these strategies, Jakarta faces challenges such as budgetary constraints, poor maintenance of existing infrastructure, and the need for significant resettlement to accommodate new projects. Moreover, the success of these initiatives requires overcoming bureaucratic and corruption-related hurdles that have historically impeded large-scale urban projects in Indonesia.\nhttps://floodlist.com/asia/plans-reduce-jakarta-flooding\nOverall, Jakarta’s approach to flood management is a blend of modern engineering and ecological restoration, aiming to safeguard the city against future flooding while also addressing broader environmental and urban resilience goals.\n:::"
  },
  {
    "objectID": "Week4.html#application-how-the-remotely-sensed-data-you-sourced-could-be-used-to-assist-with-contributing-to-the-policy-goal.-how-could-the-data-be-applied-to-solve-the-policy-challenge.",
    "href": "Week4.html#application-how-the-remotely-sensed-data-you-sourced-could-be-used-to-assist-with-contributing-to-the-policy-goal.-how-could-the-data-be-applied-to-solve-the-policy-challenge.",
    "title": "4  Policy",
    "section": "4.2 Application: how the remotely sensed data you sourced could be used to assist with contributing to the policy goal. How could the data be applied to solve the policy challenge.",
    "text": "4.2 Application: how the remotely sensed data you sourced could be used to assist with contributing to the policy goal. How could the data be applied to solve the policy challenge.\n\nFlood Mapping and Monitoring: SAR data allows for the real-time mapping of flood extents, even under cloud cover, which is crucial for timely evacuation and response.\nRisk Assessment: Combining SAR data with DEMs helps identify flood-prone areas, critical for planning and implementing flood defenses.\nInfrastructure Planning: Insights from remotely sensed data support the design and placement of flood mitigation infrastructure, like dams and improved drainage systems.\nDisaster Preparedness and Response: The data enhances the efficiency of disaster response efforts by identifying affected areas and prioritizing rescue and relief operations.\n\n:::"
  },
  {
    "objectID": "Week4.html#reflection-what-you-have-learnt-in-relation-to-the-policy-city-and-the-application-of-the-data.",
    "href": "Week4.html#reflection-what-you-have-learnt-in-relation-to-the-policy-city-and-the-application-of-the-data.",
    "title": "4  Policy",
    "section": "4.3 Reflection: what you have learnt in relation to the policy, city and the application of the data.",
    "text": "4.3 Reflection: what you have learnt in relation to the policy, city and the application of the data.\nLink to Global Goals\nJakarta’s efforts to manage flooding through remotely sensed data contribute to the Sustainable Development Goals (SDGs), particularly:\n\nSDG 11 (Sustainable Cities and Communities): By enhancing urban resilience against flooding.\nSDG 13 (Climate Action): Contributing to disaster risk reduction in the face of climate change.\n\nAdvancements Over Current Approaches\nThe use of remotely sensed data in Jakarta represents an advancement by providing comprehensive and real-time flood monitoring capabilities, supporting more informed and timely decision-making in flood management and disaster response, essential for a megacity prone to frequent and sudden flooding events.\nhttps://earthobservatory.nasa.gov/images/148303/as-jakarta-grows-so-do-the-water-issues"
  },
  {
    "objectID": "Week1.html#summary",
    "href": "Week1.html#summary",
    "title": "1  Week 1 - Getting Started With Remote Sensing",
    "section": "1.1 Summary",
    "text": "1.1 Summary\nRemote sensing is defined by NASA (quite an expert in this field) as:\n\n“the acquiring of information from a distance”\n\n\n1.1.1 Active vs. Passive Sensors\nThis is done using sensors mounted on various platforms such as satellites, planes, drones, phones, or ground/sea devices. There are two types of sensors: active and passive.\nPassive Sensors: These utilise available energy to detect reflected energy from the sun. Examples include the human eye, cameras, and satellite sensors.\nActive Sensors: Active sensors have their own energy source for illumination. They emit electromagnetic waves and then detect the energy reflected back. Examples include radar, x-ray, and LiDAR.\n\n\n1.1.2 Electromagnetic Waves: A Physics Recap\nElectromagnetic radiation (EMR) takes the form of waves and consists of electric and magnetic fields. The properties of EMR include wavelength (λ), velocity of light (c), and frequency (v), wherein the wavelength can be calculated with the following equation.\n\\(λ = \\frac{c}{v}\\)\nEMR can be absorbed, transmitted, scattered, or reflected by surfaces and particles (such as in the atmosphere). Scattering types include Rayleigh (particles are very small compared to the wavelength), Mie (particles are the same size compared to the wavelength), and non-selective (particles are much larger than the wavelength )scattering. EMR properties influence phenomena such as the appearance of the sky, ocean color, and sensor capabilities like Synthetic Aperture Radar (SAR) and polarization.\n\n\n1.1.3 Resolution\nRemote sensing systems have four main types of resolution:\n\nSpatial: Refers to the size of the raster grid per pixel, typically ranging from centimeters to kilometers.\nSpectral: Indicates the number of bands in which data is recorded across the electromagnetic spectrum. Different wavelengths reveal unique spectral signatures, but atmospheric conditions can limit observation.\nTemporal: Describes how often a sensor revisits an area. This determines pixel size and is crucial for monitoring changes over time.\nRadiometric: Identifies the range of values a sensor can detect, influencing its sensitivity to small differences in energy."
  },
  {
    "objectID": "Week1.html#application",
    "href": "Week1.html#application",
    "title": "1  Week 1 - Getting Started With Remote Sensing",
    "section": "1.2 Application",
    "text": "1.2 Application\nThe utilisation of satellite sensors, their spectral bands, and the principles of electromagnetic wave detection form the cornerstone of modern remote sensing technologies. These elements enable the comprehensive monitoring and analysis of Earth’s surface, offering invaluable data for a multitude of applications across environmental science, urban planning, disaster management, and agriculture. It is hard to imagine how remote sensing is not useful - it seems to form the backbone of many vital modern services. Much GIS analysis and strategic planning relies on satellite data. It is this data that tackling many of the world’s issues hinges on: climate change responses and uderstanding could not take place (at least to the extent they now do) without this data; the level of objective precision it provides is vital. Additionally, for understanding optimal location services (whether governmental, commercial, or otherwise), these datasets are vital, such as for understanding soil types at a large-scale, or conducting regional, national or even planetary analysis and research.\n\n1.2.1 Satellite Sensors and Spectral Bands\nActive sensors, such as Synthetic Aperture Radar (SAR), emit their own electromagnetic radiation and measure the energy that is reflected back from Earth’s surface. This capability allows SAR sensors, such as in Sentinel-1, to acquire data regardless of daylight or weather conditions, making them especially useful for monitoring changes in surface topography, vegetation structure, and urban development.\nPassive sensors, on the other hand, rely on sunlight as their energy source, detecting the radiation reflected or emitted by objects on Earth. These sensors, found on platforms like Landsat and Sentinel-2 satellites, capture data across multiple spectral bands, including visible light, infrared, and near-infrared. Each band is sensitive to different surface properties, enabling the detailed observation of vegetation health, water quality, and land cover changes.\nThe concept of spectral signatures, unique to each material or object, is fundamental to remote sensing. By analysing the way different surfaces absorb and reflect light across various bands, scientists can identify and classify the materials present on Earth’s surface with remarkable accuracy. This capability is pivotal for creating land cover maps, assessing crop health in agriculture, and monitoring environmental degradation.\n\n\n1.2.2 Environmental Monitoring\nStudies leveraging the spectral bands of passive sensors have significantly advanced our ability to monitor environmental changes. For instance, the NDVI (Normalized Difference Vegetation Index), which uses red and near-infrared bands, has been widely applied to assess vegetation health and biomass. This index has facilitated global monitoring of deforestation, desertification, and the impacts of climate change on ecosystems. This is evident with Global Forest Watch, an initiative that uses satellite imagery, including Landsat and Sentinel-2 data, to monitor forest changes globally in real-time.\n\n\n1.2.3 Urban Planning\nActive sensors like SAR have revolutionized urban planning through their ability to penetrate cloud cover and provide high-resolution images of urban areas. The data from SAR sensors support the mapping of urban infrastructure, monitoring of construction activities, and assessment of urban sprawl, contributing to more informed planning and development policies.\n\n\n1.2.4 Disaster Management\nThe real-time data acquisition capabilities of satellite sensors are crucial for disaster management. SAR imagery, for example, has been instrumental in mapping flood extents and assessing damage post-disaster. This information enables rapid response and aids in the efficient allocation of resources during emergency situations.\n\n\n\n1.2.5 Agriculture\nThe multispectral data from passive sensors are extensively used in precision agriculture to monitor crop health, optimise irrigation, and predict yields. By analysing spectral signatures, farmers can identify stressed crops early and apply targeted interventions, leading to increased productivity and sustainability."
  },
  {
    "objectID": "Week1.html#relfection",
    "href": "Week1.html#relfection",
    "title": "1  Week 1 - Getting Started With Remote Sensing",
    "section": "1.3 Relfection",
    "text": "1.3 Relfection"
  },
  {
    "objectID": "Week2.html#landsat-5-profile",
    "href": "Week2.html#landsat-5-profile",
    "title": "2  Using Xaringnan",
    "section": "2.1 Landsat 5 Profile",
    "text": "2.1 Landsat 5 Profile\n\n\n\n\n\n\n\n\n\nHere is a presentation about the Landsat 5 satellite!"
  },
  {
    "objectID": "Week3.html#summary",
    "href": "Week3.html#summary",
    "title": "3  Corrections and Enahancements",
    "section": "3.1 Summary",
    "text": "3.1 Summary\n\n3.1.1 Pre-processing\nPre-processing Requirements: Remote sensing imagery may have errors due to factors like sensor issues, atmospheric conditions (such as clouds, rain, ash, etc), terrain, among others. These can be adjusted for.\nScan Lines Issue: An example of a sensor issue is the failure of the scan line corrector on Landsat 7, leading to gaps in imagery that necessitated development of gap-filling methods.\nRegression: This concept is important for understanding how to model and predict values in remote sensing data. \\(yi=β0+β1xi+ϵi\\)\n\n\n3.1.2 Corrections\nGeometric Correction: Corrects for image distortions due to view angle, topography, wind, and Earth’s rotation using Ground Control Points (GCPs) and geometric transformation coefficients.\nAtmospheric Correction: Essential for removing atmospheric effects that can alter image data, with methods ranging from simple dark object subtraction to complex radiative transfer models.\nOrthorectification Correction: Refines georectification by removing distortions to ensure each pixel is represented as if viewed directly from above.\nRadiometric Calibration: Transforms sensor-captured image brightness into meaningful spectral radiance measurements.\n\n\n3.1.3 Joining Data Sets\nJoining Data Sets (Mosaicking): Techniques to merge multiple images seamlessly, often requiring adjustments to ensure consistency across the combined imagery.\n\n\n3.1.4 Image Enhancement\nBasic Enhancements: improves the visual quality and interpretability of imagery\n\nContrast enhancement\nBand ratio Examples include NDVI (Normalised Difference Vegetation Index), NDWI (Normalised Difference Water Index), SAVI (Soil-Adjusted Vegetation Index).\n\n\\[NDVI = \\frac{NIR-Red}{NIR+Red}\\]\n\\[NDWI = \\frac{NIR-SWIR}{NIR+SWIR}\\]\n\\[SAVI = \\frac{NIR-Red}{NIR+Red+L}×(1+L)\\]\n\n\\(L\\) = value is adjusted based on the amount of vegetation; \\(L\\)=0.5 is the default value and works well in most situations. With \\(L\\)=0, NDVI=SAVI.\n\n\nFiltering\nEdge enhancement\n\nAdvanced Enhancements: further refines and enhances imagery for specific applications.\n\nPrincipal Component Analysis (PCA)\nTexture analysis\nImage fusion (e.g. pan sharpening)\n\nKey Considerations The process involves correcting imagery for various errors before using it for analysis or interpretation.\nEnhancements and corrections aim to improve image quality and consistency, facilitating better comparison and analysis across different images or time periods.\nThe application of these processes needs to be tailored to the specific needs and contexts of the imagery use, particularly in complex environments like urban areas.\nAdvanced techniques can add value to the imagery but also introduce complexity. It’s essential to weigh the benefits against the potential complications or misinterpretations that may arise."
  },
  {
    "objectID": "Week3.html#application",
    "href": "Week3.html#application",
    "title": "3  Corrections and Enahancements",
    "section": "3.2 Application",
    "text": "3.2 Application\n1. Loading Tiles\nIn the below I have downloaded two satellite image collections from USGS. They pertain to an area of Northern France, part of the English Channel, and South-East England.\n\n\n\n\n\nLoaded Tile (north) - ‘LC08_L2SP_200025_20230604_20230607_02_T1’; displaying the raw bands\n\n\n\n\n\n\nLoaded Tile (south) - ‘LC08_L2SP_200026_20230604_20230607_02_T1’; displaying the raw bands\n\n\n\n\n\nHere we can see the different band values (the ‘layers’ of the images, reflecting the levels of reflectance of that correlating wavelength) for the individual tiles\n\n2. Merge Tiles\n\n\n\nMerged Tile\n\n\n\nHere we can see the different band values for the merged tiles together\n\n3.1 Enhancements: Ratio\n\n\n\nRatio Enhancement - NVDI\n\n\n\nValues are calculated using the NDVI formula above. Band 4 is Near-Infrared, while Band 5 is Red\n\n\n\n\nRatio Enhancement - NVDI Greater Than 0.2; Merged Tile\n\n\n\nHere I have pulled out the values where the NDVI is greater than 0.2 (typically regarded as a good threshold level for healthy vegetation). Values less than 0.2 are simply displayed as white. We can see the difference between this and the first image generated for NDVI; especially when looking at water - no water is highlighted (due to not reflecting enough of those bands), consequently we are left with an image which describes the spatial arrangement of healthy vegetation.\n\nClipping the Merged Tile to a Study Area\n\n\n\nSample Area; clipped from merged tile\n\n\n3.2 Enhancements: Texture\n\n\n\nTexture Enhancement - Sample Area\n\n\n\nHere a texture enhancement has been performed on the study area. Higher values (closer to 1) from a texture analysis suggest that the area is more homogeneous (less contrast in the pixel values and a consequential smoother texture). This thus indicates homogeneous areas like open water, fields, or smooth and barren land, depending on the context of the image and other geographic data. The green areas have higher values, which seems appropriate; they largely reflect rural France which is comprised greatly of open fields.\n\n3.2 Enhancements: Fusion\n\n\n\nPCA Enhancement - Sample Area\n\n\n\nThe fusion performed here has stacked the raster layers, utilising the clipped image (the clipped study area) and the texture enhancement, on top of each other to create a multi-layer raster stack or a multi-band raster object. By comparing the homogeneity layer with the spectral layers, we can identify areas where the spectral data alone may not have been as informative. For example, if a green area in the homogeneity layer corresponds with a unique feature in one of the spectral layers, this could indicate a region of interest for further investigation.\n\n3.3 Enhancements: PCA\n\n\n\nPCA Enhancement - Sample Area\n\n\n\nPCA transforms the original correlated variables into a new set of uncorrelated variables, called principal components (PCs), which are ordered so that the first few retain most of the variation present in the original dataset. Essentially it can be used to reduce the number of dimensions of a dataset. The first image is the one of most interest here - that new dimension effectively helps to explain most of the variance amongst other dimensions."
  },
  {
    "objectID": "Week3.html#reflection",
    "href": "Week3.html#reflection",
    "title": "3  Corrections and Enahancements",
    "section": "3.3 Reflection",
    "text": "3.3 Reflection\nAfter discussing with a few coursemates, many people seemed to struggle with the content of this week, however, I must say that I quite enjoyed it. The process of correcting an image feels somewhat linear and I regret not producing a flow chart to really demonstrate its relative simplicity.\nAs with the first week - it is quite fun and satisfying to take quite illegible images and turn it into something actionable and insightful. The sample area that I looked at this week (when I clipped my merged tile to a smaller bounding area) contains Calais, France and its surrounding landscape. It is evident from the analysis I have conducted that it is predominantly countryside uses - high NDVI values and an abundance of high values within the texture enhancement. However, the texture analysis also revealed an ‘archipelago in the sea of green’ of moderately textured areas. From looking at OS maps and through my own understanding I would assume that these represent very sparse human settlements (towns and villages as well as agricultural built-up uses).\nRatio seems like an exceptionally easy to use and understand concept in particular (unless I have completely understoon it!). There seem to be a vast array of indices (and their formulae) available and easily searchable online. It primarily a matter of taking the band values for a pixel (dependent on the index you are attempting to calculate) and applying the function to the image.\nThis is a highly relevant topic and something I will likely return to in my own time to play with. When it comes to the matter of professional use, however, it does slightly concern me. Whilst I feel I am able to draw basic insights from manipulated imagery, identifying more nuanced phenomena seems much harder. Although I am sure that this is mostly a matter of exposure and with enough use, I will grow familar with what to look out for, what is and isn’t typical, etc."
  },
  {
    "objectID": "Week6.html#summary",
    "href": "Week6.html#summary",
    "title": "5  Intro to Google Earth Engine",
    "section": "5.1 Summary",
    "text": "5.1 Summary\n\n5.1.1 The Setup of GEE\nGoogle Earth Engine (GEE) is a geospatial processing service that allows for analysis at a large scale, leveraging massive datasets for planetary-scale analysis quickly. It operates by storing data on servers and executing user-written code on this data.\nGEE Terminology:\n\nImage: Represents raster data and contains bands.\nFeature: Represents vector data and contains geometry and attributes.\nImageCollection: A stack of images.\nFeatureCollection: A collection of multiple polygons or vector data.\n\nGEE uses JavaScript for coding, introducing specific syntax for variables and objects.\nClient vs. Server Side: Code runs either in the browser (client-side) or on the server where data is stored. Objects prefixed with ee are server-side and considered “proxy objects.”\nScale and Projections:\nScale refers to the resolution of images and is determined by the output in GEE, with automatic resampling as needed.\nProjections are managed automatically by GEE, typically converting data into the Mercator projection for display.\n\n\n5.1.2 GEE in Action\nBuilding Blocks of GEE: Objects can be vectors, rasters, features, strings, or numbers, each belonging to a specific class with unique GEE functions.\nImage Collections: These are managed by filtering based on dates and regions to handle vast amounts of raster data.\nGeometries and Features: Geometries are points, lines, or polygons without attributes, whereas features include attributes.\nCommon GEE Processes: Include geometry operations, joins, zonal statistics, filtering, and various machine learning methods.\nReducing Images: This involves summarizing image collections to specific values per pixel or region, such as calculating median values or average reflectance within a study area.\nRegression Analysis: GEE supports linear regression over time to analyze changes in pixel values, utilizing a least squares approach and potentially multiple dependent variables.\nJoins: Similar to database joins, allowing for the combination of image or feature collections based on specific criteria."
  },
  {
    "objectID": "Week6.html#application",
    "href": "Week6.html#application",
    "title": "5  Intro to Google Earth Engine",
    "section": "5.2 Application",
    "text": "5.2 Application\n\nThe introduction of Google Earth Engine has transformed the ways in which geospatial data is and can be analysed and applied. Due to its easy access to data (with over 50Pb of data in its catalogue!), processing capabilities, as well as its commercial accessibility, GGE has significant applications in environmental and climate change science, urban planning, agriculture, and disaster management, among many other uses.\nIn 2022, Google made it possible to buy commercial licenses for the use of the platform, before this it was restricted (formally) for use in research, education, or not-for-profit activities.\n\n\n\nGoogle Earth Engine Scholarly Use Over Time (Zhao et al, 2021)\n\n\n\n5.2.1 Environmental Monitoring and Conservation\nOne of the most significant applications of GEE is in environmental monitoring and conservation. Because of the platform’s ability to handle massive datasets and perform planetary-scale analyses has been pivotal in tracking deforestation and forest fires, mapping biodiversity, and monitoring water resources. A study by Hansen et al. (2013) utilised GEE to map global forest cover and loss, providing insights into deforestation trends that inform conservation policies. They produced high-resolution global maps of 21st-century forest cover change. The study used GEE’s ability to handle large-scale image collections and its efficient processing capabilities (which run on powerful remote servers) to analyse satellite imagery over time, illustrating the platform’s power in environmental governance and policy-making. This ability to more easily understand, map, and quantify the world.\n.\n\n\n5.2.2 Climate Change and Carbon Sequestration Studies\nGEE has also been instrumental in climate change research, particularly in assessing carbon stocks and fluxes. The platform’s ability to process and analyze vast amounts of raster data enables researchers to estimate carbon sequestration and emission levels across large areas. For example, Gorelick et al. (2017) highlighted the use of GEE in quantifying changes in forest carbon stocks, employing the platform’s image collection and reduction capabilities to assess temporal changes in biomass. Such studies are crucial for understanding the carbon cycle and informing international climate agreements like the Paris Agreement.\n\n\n5.2.3 Agricultural Practices and Sustainability\nSome companies, such as Regrow, are using GEE to report and verify regenerative and sustainable agricultural techniques. Through GEE’s analysis of historical and satellite imagery, Regrow is able to generate detailed data on farmland, enhancing the ability to monitor and improve sustainable practices on a large scale.\nRegrow has collaborated with General Mills to track the adoption of regenerative agriculture practices across 175 million acres of agricultural land, globally. This effort involves using remote sensing technology and modeling (employing GEE) to monitor and verify regenerative techniques. The partnership is aiming to enhance resilience in General Mills’ supply chain by making well-informed decisions about investing in regenerative programs, with the aim of reducing greenhouse gas emissions by 30% by 2030 (although, in my personal view, this 30% target does not seem particularly revolutionary or impressive; the UK government has set its target at 68%, though the baseline level for each may vary).\nRegrow and General Mills utilised the Operational Tillage Information System (OpTIS) and DeNitrification-DeComposition (DNDC) models for their project. OpTIS leverages satellite imagery to monitor and verify regenerative agriculture techniques, including tillage reduction and cover-crop adoption. The data from OpTIS feeds into the DNDC model, which simulates nutrient cycling in soil to predict crop growth, soil temperature and moisture, carbon dynamics, nitrogen leaching, and trace gases emissions and thus estimate net changes in greenhouse gas emissions."
  },
  {
    "objectID": "Week6.html#reflection",
    "href": "Week6.html#reflection",
    "title": "5  Intro to Google Earth Engine",
    "section": "5.3 Reflection",
    "text": "5.3 Reflection\nGoogle Earth Engine certainly makes using RS data easier to manage. Its speed is incomparable with SNAP - where SNAP is walking, GEE is whizzing around the planet at the speed of the satellites themselves it would seem!\nWhilst GEE is largely easy to use, it does come with the caveat of learning another coding language. As it is based on JavaScript, however, this does not seem too bad and shares many similarities with other languages I have learned, also the actual amount of JavaScript knowledge that is required is relatively little.\nWhat has really surprised me is the immense uptake of GEE use within publications - annoyingly I can’t seem to find the exact time-series plot I seem to remember us seeing in the lecture (of GEE use in RS publications compared with other languages uses), but the one I have shown for this week’s entry still demonstrates the point - it has received exponentially increasing uptake, which is only likely to increase as more people become familiar with it.\nIt also surprised me that GEE only became available for commercial use in 2022 - given its immense capabilities, it seems (from a profit point of view) crazy for Google not to be licensing out the software’s use. Although, I would assume perhaps Google was anxious of too significant interest that its servers (renown for their speed, compared to local computational capacity) would become overwhelmed.\nOn the note of profitability, whilst its uptake is massively impressive, it does raise concerns (at least in my mind) of monopolisation - if Google come to dominate the RS space and industry there are no doubt consequences of this. They have demonstrated themselves to deliver high-quality and have an exceptionally well regarded reputation. However, Carr and Hesse (2020) demonstrated the implications of Alphabet Inc.’s (Google’s parent company) involvement in a ‘smart city’ development in Toronto, Canada. They behaves in a hegemonic, monopolisitc way, disregarding local issues and complaints. Whilst that was within the realm of urban planning and urban development, it is still concerning."
  },
  {
    "objectID": "Week7.html#summary",
    "href": "Week7.html#summary",
    "title": "6  Classification I",
    "section": "6.1 Summary",
    "text": "6.1 Summary\nThis week has focussed on the application and classification of remotely sensed data, specifically looking into urban expansion, air pollution, land use and land cover (LULC) changes, urban green spaces, forest monitoring, and fire detection.\nIt incorporates the use of various sensors like Landsat, Sentinel-3, and Sentinel-5, along with different methodologies including regression analysis, hex grids (honeycombing), and statistical comparisons.\n\n6.1.1 Urban Expansion and Air Pollution\nThe impact of LULC changes on air pollution is examined through major air pollutants (MAP) and land surface temperature (LST) using sensors and regression analysis. The case study used highlights the LULC distribution’s significant effect on MAP and LST in Iran, relying on data from the National Cartographic Center.\n\n\n6.1.2 Urban Green Spaces\nDifferent techniques and sensors are used to study urban green spaces, indicating a mix of methodologies like hybrid methods, object-based image analysis, and various mapping purposes. The importance of accurate mapping and classification of urban green areas is emphasized through inventory and assessment, ecosystem services, and species mapping.\n\n\n6.1.3 Monitoring Forests and Illegal Logging\nChallenges in monitoring illegal logging in Brazil’s Amapá are discussed, including limited resources and the utilization of tools like Global Forest Watch and Landsat for identifying deforestation areas. Also, forest loss monitoring techniques, such as pre-processing Landsat images and creating metrics for classification and analysis.\n\n\n6.1.4 Machine Learning and Classification Techniques\nAn exploration of expert systems, machine learning principles, and the role of inductive learning in environmental data analysis. Detailed discussion on classification and regression trees (CART), including the concept of Gini impurity, decision trees, and random forests for handling complex environmental datasets. Introduction to image classification techniques, both supervised and unsupervised, including maximum likelihood classification and support vector machines (SVM), highlighting their applications in remote sensing data classification.\n\n6.1.4.1 CART\n\n\n\nExample of CART process (geeksforgeeks)\n\n\nWhat? A predictive model that represents a decision process for classification or regression. It’s used to categorise or predict the value of a target variable based on input variables (such as different spectral bands).\nHow? It creates a tree-like structure where each node represents a decision rule on an input variable, and each branch represents the outcome of that decision leading to different predictions or classifications.\n\n6.1.5 Gini Impurity\nWhat? A measure used in CART to determine how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset.\nHow? It measures the frequency at which any element of the dataset will be misclassified when it is randomly labeled. A Gini Impurity of 0 means all elements belong to a single class, indicating perfect purity.\n\n\n\n\n6.1.6 Regression Trees\nWhat? A type of decision tree designed for continuous data. While classification trees are used to predict categorical outcomes, regression trees predict numeric outcomes (such as the amount of rainfall).\nHow? It splits the data into branches to minimize the variance of responses within the branches, aiming to create groups that are as homogenous as possible in terms of the continuous response variable.\n\n\n6.1.7 Random Forests\nWhat? An ensemble learning method that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\nHow? It improves the predictive accuracy and controls over-fitting by averaging or combining the results of individual decision trees, which reduces the variance and bias of the model.\n\n\n6.1.8 Unsupervised Machine Learning (K-Means)\nWhat? An unsupervised learning algorithm that groups unlabeled datasets into different clusters. K-means clustering is a method to automatically partition the data into K groups.\nHow? It identifies K centroids, and then allocates every data point to the nearest cluster, while keeping the centroids as small as possible.\n\n\n6.1.9 ISODATA (Iterative Self-Organizing Data Analysis Technique)\nWhat? A more advanced version of K-means that allows the number of clusters to be automatically adjusted during the iteration process.\nHow? It starts with an initial set of clusters and iteratively refines them by splitting, merging, or removing clusters based on the variability of the data and the distance between clusters.\n\n\n6.1.10 Supervised Learning (Parametric and Non-Parametric)\nWhat? Supervised learning involves training a model on a labeled dataset, which means the model learns from data that already contains the answers.\n\nParametric methods assume a predefined form for the function that maps inputs to outputs (e.g., linear regression).\nNon-Parametric methods do not assume any specific form for the mapping function, allowing the data to determine the model structure (e.g., k-nearest neighbors).\n\n\n\n6.1.11 SVM (Support Vector Machines)\nWhat? A classification method that works by finding the hyperplane that best separates different classes in the feature space.\nHow? It looks for the hyperplane with the maximum margin, meaning the greatest distance between data points of both classes. It can handle linear and non-linear data thanks to the use of kernel functions that implicitly map input features into high-dimensional spaces."
  },
  {
    "objectID": "Week7.html#application",
    "href": "Week7.html#application",
    "title": "6  Classification I",
    "section": "6.2 Application",
    "text": "6.2 Application\n\n6.2.1 Random Forests in Land Cover Classification\nThe use of Random Forests (RF) for land cover classification means that more accurate maps of land use and cover can be developed, allowing policymakers to better assess the status of natural habitats, plan conservation areas, and monitor changes over time, as well as many other uses related to urban planning, economic and environmental development, etc. Its application has supported biodiversity conservation policies by identifying critical habitats and tracking the effectiveness of conservation measures.\nRodriguez-Galiano et al (2012) explore the effectiveness of the RF classifier for land cover classification in a notably complex area, using multi-temporal Landsat-5 Thematic Mapper data and variables from a DEM. The study area focused on the Province of Granada in the south of Spain, which is characterised by a diverse range of land cover types. Random Forests is shown to provide a high classification accuracy, the ability to handle large datasets with noise, and its flexibility across different types of data analysis without requiring assumptions about data distribution.\nSome key findings from the study include:\n\nThe RF classifier achieved a high level of mapping accuracy for land cover classification, reaching an overall accuracy of 92% and a Kappa index of 0.92 (indicating excellent agreement beyond chance).\nRF’s robustness was notable against reductions in training data size and the addition of noise. Significant decreases in accuracy were observed only when training data was reduced by more than 50% and noise was added above 20%.\nIt was identified that texture and geostatistical measures (such as madograms, rodograms, direct variograms, cross- and pseudo-cross variograms), alongside multi-seasonal spectral information, are the most important variables for classification, which had aligned with prior expectations. This helps in understanding the factors influencing land cover types.\nCompared to a single decision tree, RF demonstrated superior performance, validated by a McNemar test with a significance level of 0.00001, suggesting that RF is a more reliable method for land cover classification in complex areas.\n\nThe study underscores the value of RF as an ML technique for land cover monitoring, particularly in large, heterogeneous landscapes. Its ability to maintain high accuracy despite data challenges and its non-parametric nature make it a powerful tool for remote sensing applications. This work not only validates the efficacy of RF in classifying complex land cover types but also encourages its broader adoption in the remote sensing community for large-scale environmental monitoring and analysis."
  },
  {
    "objectID": "Week7.html#reflection",
    "href": "Week7.html#reflection",
    "title": "6  Classification I",
    "section": "6.3 Reflection",
    "text": "6.3 Reflection\nThese are clearly rather powerful methods. Some, such as SVM and RF seem to be more so than others, though it is difficult to precisely tell at this stage - I hope that as I ustilise them further, I will gain a greater appreciation for their specific use cases and the differences between exactly what their outputs are telling me (without having to scramble around on Google!).\nThis is my first time properly engaging with ML techniques and I have heard much about issues with biases in training data - the data you feed the algorithm is the data it then uses to conduct analysis from which insights are derived. Upon seeing its use in RS, I am reminded of this issue - if we trained a model solely on data from the Saraha Desert and then tried to apply it to the Amazon Rainforest, you would probably see very ‘wild’ outputs. This is probably a slight exaggeration of what most people would think to do, but it explains the issue well. I was recently in Denmark at the Lousiana Museum of Modern Art where an installation called From ‘Apple’ to ‘Abomination’ by Trevor Paglen (see pictures at the bottom of the page). It demonstrated the vitality of training ML algorithms without bias.\n\nFor example, defining what an apple looks like is relatively simple, but how do we define what a ‘good’ or ‘bad’ person looks like?\n\nThis is highly relevant to RS where the question of how do we define what something like an urban settlement looks like? Is there one form? Are there many? If we assume that a settlement must have roads, what of smaller villages located in more rural areas (especially in lesser developed countries) where they may not have roads. A city in Niger looks very different to a city in Brazil, which looks very different from a city in the US. This is probably where methods, such as unsupervised ML techniques are most useful - applying these techniques to global dataset (due to the nature of satellite imagery which captures the whole Earth), much less human interaction and bias can be introduced to the model, maintaining its objectivity as well as can be done."
  },
  {
    "objectID": "Week8.html#summary",
    "href": "Week8.html#summary",
    "title": "7  Classification II",
    "section": "7.1 Summary",
    "text": "7.1 Summary\n\n7.1.1 Landcover Classification (Continued)\nPre-classified Data Sources:\n\nGlobeLand30 provides 30m resolution data for 2000, 2010, and 2020.\nESA’s Climate Change Initiative offers annual global land cover at 300m resolution from 1992-2015.\nDynamic World offers near real-time 10m resolution data.\nOther sources include MODIS and Google’s building data.\n\nDynamic World:\n\nUtilizes a semi-supervised approach with the world divided into three regions and 14 biomes.\nStratified samples are based on NASA’s MCD12Q1 land cover data for 2017 among other sources.\nLabeling involves both expert and non-expert groups, with a focus on achieving a balance in confidence levels across different classifications.\n\n\n\n7.1.2 Accuracy\nObject-Based Image Analysis (OBIA):\n\nFocuses on representing ground objects as shapes (superpixels) based on their homogeneity or heterogeneity.\nThe SLIC algorithm is a common method for superpixel generation, balancing between physical distance and color similarity.\n\nSub-Pixel Analysis:\n\nAddresses the issue of pixels composed of multiple land cover types by determining the proportion of each land cover within a pixel.\n\nAccuracy Assessment:\n\nInvolves assigning an accuracy value to the output of remote sensing analysis, focusing on Producer’s Accuracy, User’s Accuracy, and Overall Accuracy.\nThe Kappa Coefficient and F1 Score are among the metrics used to evaluate accuracy, with considerations for the balance between recall and precision.\nThe Receiver Operating Characteristic (ROC) Curve and Area Under the ROC Curve (AUC) are used to assess the performance of binary classifiers.\n\nSpatial Considerations in Accuracy Assessment:\n\nSpatial cross-validation and the consideration of spatial autocorrelation are highlighted as important factors in ensuring the reliability of classification models.\nThe need for spatially aware methods like spatial leave-one-out cross-validation (SLOO CV) and k-fold cross-validation with spatial partitioning is emphasized."
  },
  {
    "objectID": "Week8.html#application",
    "href": "Week8.html#application",
    "title": "7  Classification II",
    "section": "7.2 Application",
    "text": "7.2 Application\n\n7.2.1 A Deep Convolution Neural Network Method for Land Cover Mapping\nA Case Study of Qinhuangdao, China\nHu et al (2018) address several challenges in land cover classification using traditional CNN methods, such as the application to multispectral and hyperspectral imagery and the automation of training dataset construction. They introduce a novel deep CNN (DCNN) based on Landsat-8 imagery, which integrates cascaded cross-channel parametric pooling and global average pooling methods. Their adapted approach allowed the researchers to well utilise multispectral data from the Landsat-8 OLI. This approach is directly responding to the need for models that are able to better generalise and handle the complex nature of satellite imagery, including the challenge of automating the construction of a training database.\nThe study introduces a semi-automatic method for constructing these datasets, which significantly reduces the need for manual labour (such as manually labelling training data - a long, slow, and arduous task!). It leverages existing high-quality land cover products (such as comprehensive maps or datasets that have already been accurately classified). By extracting land type information from these products, the method can automatically assign labels to training data points based on their location and the corresponding classification in the high-quality land cover product.\n\nFor example, if a satellite image pixel falls within a region classified as “forest” in a high-quality land cover map, the semi-automatic method can automatically label that pixel as “forest” in the training dataset. This approach allows for the creation of large, accurately labeled training datasets at a greater pace than other methods, enabling more efficient training of DCNNs for tasks like land cover classification.\n\nThis addresses a major bottleneck in applying machine learning to remote sensing, which is the task of preparing large-scale, accurately labelled datasets. By automating part of the labeling process, researchers can more readily apply advanced machine learning models to the vast amounts of satellite imagery available, potentially leading to better models for predicting and understanding land cover changes on a global scale.\nThe DCNN model demonstrated superior performance over traditional methods, achieving an overall accuracy of 82.0% and a kappa coefficient of 0.76. This improvement is attributed to the model’s ability to incorporate spatial information from surrounding pixels (vital for addressing spatial autocorrelation which is inherent in spatial data), reducing misclassifications that often occur in pixel-based approaches. Typical pixel-based approaches might treat each pixel as independent, ignoring the potentially valuable context provided by adjacent pixels. The DCNN employed here is able to accurately distinguish between land uses (such as in areas where the boundaries between different land covers are more graduated or where classes have similar spectral signatures but are spatially distinct). The overall accuracy value and kappa coefficient highlight the significant improvements that can be attained through deep learning models when compared with other classification methods such as SVM and MLC."
  },
  {
    "objectID": "Week8.html#reflection",
    "href": "Week8.html#reflection",
    "title": "7  Classification II",
    "section": "7.3 Reflection",
    "text": "7.3 Reflection\nAs discussed last week, ML takes quite a central themes of this week and indeed is a very big and exciting subject within classification. The case study of Qinhuangdao demonstrates the improvements that can be achieved when employing unsupervised techniques and when also accounting for inherent methodological issues. It is only semi-supervised though (rather than fully unsupervised), with it utilising an unsupervised method of classifying pixels, through existing datasets trained under supervision.\nSomething that I have noticed though is that I see the lectures and the methods being taught within these (e.g. sub-pixel analysis or CNN). During these, the methods seems somewhat straightforward (although a constant theme is the complexity involved in many aspects of RS). But then when I research, it becomes clear that there is an extensive array of variations, interpretations and adaptations of the methods. Initially, this struck me as daunting - how could I ever learn all of these and make sense of them. But through reading some more, it is evident that there are general principles which are constantly regarded. For example, variations of a method may simply involve changing threshold levels or minor process tweaks. Instead it is actually exciting - that there is so much interest and such burgeoning adaptability, demonstrates the will that researchers posses to explore new possibilities and identify more accurate, precises and efficient methods. It demonstrates that the RS community still has much to discover, learn and discuss."
  },
  {
    "objectID": "Week1.html#reflection",
    "href": "Week1.html#reflection",
    "title": "1  Week 1 - Getting Started With Remote Sensing",
    "section": "1.3 Reflection",
    "text": "1.3 Reflection\nThis week has introduced the fundamental ideas behind remote sensing. My initial thoughts are that it is quite daunting. It becomes easy to get confused between different satellites, their capabilties, their exact properties, etc. However, it is vital to so many processes. The fight against climate change seems heavily reliant on remotely sensed data and analysis and indeed many examples of remote sensing technology use is in regard to forest monitoring, flooding, agricultural management and land classification.\nThis week introduced SNAP and it is safe to say that I look forward to getting stuck into Google Earth Engine. Snap, due to running locally, can often take a while to process data and provide outputs, which can prove to be rather cumbersome. However, it is is quite fun to play with. Taking something that intially looks illegible and, frankly, scary into something useful and actionable gives a sense of accomplishment which somewhat makes the process worth it!"
  },
  {
    "objectID": "Week9.html#summary",
    "href": "Week9.html#summary",
    "title": "8  SAR",
    "section": "8.1 Summary",
    "text": "8.1 Summary\n\n8.1.1 SAR Fundamentals\nSAR (Synthetic Aperture Radar):\nAn active sensor technology that emits electromagnetic signals to capture surface texture data, capable of seeing through weather and clouds. SAR sensors can be mounted on various platforms and utilize different wavelengths for diverse applications. It actively emits and then receives electromagnetic waves, distinguishing it from passive sensors that rely on available energy, such as the sun’s reflection.\nPolarization:\nRefers to the orientation of the electromagnetic wave’s plane, critical in determining how different surfaces interact with SAR signals. Polarisations include single (horizontal or vertical), dual, and various combinations (HH, VV, VH, HV), influencing how rough surfaces, vegetation, or man-made structures are detected.\nAmplitude and Phase:\nSAR signals contain amplitude (backscatter) information related to surface properties and phase data indicating the wave’s cycle position upon return. This information is crucial for understanding surface roughness, volume, and permittivity.\n\n\n8.1.2 Practical Change Detection with SAR\nInSAR and DInSAR:\nTechniques leveraging SAR images to detect surface topography and motion. Interferometry uses phase shifts to map ground movement, while Differential Interferometry (DInSAR) isolates movement by removing topographic effects.\nSAR Data Processing:\nInvolves transforming complex data (I and Q components) into a form suitable for analysis or visualization, presented in power, amplitude, or dB scale, each serving different purposes from statistical analysis to visual representation.\nChange Detection Methods:\nTraditional methods like image subtraction are less effective due to SAR statistics. Ratio and log ratio techniques, improved ratio methods, and statistical tests (e.g., t-tests) are employed to better handle SAR data’s unique properties.\nImage Fusion:\nCombines SAR with optical data to enhance analysis, utilizing techniques like Principal Component Analysis and Object-Based Image Analysis for more accurate change detection and classification.\nApplications and Challenges:\nSAR’s ability to penetrate clouds and capture data under any weather conditions makes it invaluable for monitoring changes in the Earth’s surface. However, interpreting SAR images, especially in urban areas, can be complex due to signal scattering phenomena like multiple bouncing and shadowing.\n\n\n8.1.3 Key Takeaways\n\nSAR technology offers a powerful tool for Earth observation, capable of providing critical data on surface characteristics and changes over time, unaffected by weather conditions.\nIts applications range from environmental monitoring to urban planning, leveraging various data processing and change detection techniques to interpret the complex signals captured by SAR sensors.\nDespite its advantages, the effective use of SAR data requires careful consideration of sensor characteristics, data processing methods, and the specific goals of the analysis to overcome inherent challenges in interpretation and application."
  },
  {
    "objectID": "Week9.html#application",
    "href": "Week9.html#application",
    "title": "8  SAR",
    "section": "8.2 Application",
    "text": "8.2 Application\n\n8.2.1 Rice Crop Monitoring\nIn Japan, a study leveraged C-band microwave images from Sentinel-1 satellites to monitor rice crop growth. The research aimed to understand how microwave backscatter behavior is influenced by decreases in panicle water content during ripening, which affects C-band backscatter. The findings demonstrated that Sentinel-1 images with shallow incidence angles and high revisit capabilities hold significant potential for estimating panicle water content, thereby providing a method to estimate the proper harvesting time based on grain filling conditions​​.\n\n\n8.2.2 Wetland Monitoring\nA comprehensive meta-analysis revealed SAR’s capability in wetland monitoring, emphasising its unique potential for rapid and accurate mapping of wetland extent and type. This study reviewed 172 papers, and highlighted an upward trend in the use of SAR data for wetland research, with attention to the benefits of integrating multi-sensor data and the advantages of using multi-frequency and multi-polarised SAR data for higher classification accuracy.\n\n\n\nNumber of studies for wetland mapping, change detection, water level monitoring, inundation mapping (water extraction), biomass and soil moisture application are shown (Adeli et al, 2020)\n\n\nThe study also identified challenges in applying SAR for wetland monitoring, such as the backscattering similarity among different wetland classes and the selection of appropriate SAR specifications based on wetland type. They suggest that the issue of backscattering similarity is best addressed through the use of supervised machine learning techniques (such as SVM, MLC, or Object-Based Random Forests).\n\n“Changes in wetlands alter the surface being illuminated by the SAR signal and consequently change the image intensity or backscattering mechanism. The final outcome of intensity based change detection technique is calculated by differencing before and after phenomena image pixels. As mentioned, a change in the backscattering mechanism can be considered as an indicator of change in the surface type. As a case in point, when a wet soil turns into an open water body the backscattering mechanism changes from surface scattering to specular scattering. If the wetland surface is more complex, polarimetric features can be used as an input to object-based classification.”\n- Adeli et al (2020, p. 17)"
  },
  {
    "objectID": "Week9.html#reflection",
    "href": "Week9.html#reflection",
    "title": "8  SAR",
    "section": "8.3 Reflection",
    "text": "8.3 Reflection\nWhat struck me this week is the ability of SAR to detect micro changes. Due to the properties of the waves being used (very small wavelengths), through implementing change detection (looking at one period in time compared to another), it is possible to detect cm changes in the elevation of the land.\nI also find it quite jarring to move between SAR (for example) and optical waves (e.g. as collected by Landsat) - it would appear there are slightly different techniques involved. This is obvious though given they are different techniques. I have focussed on Landsat data for most of the module and have built a relative comfort to it, and so to then move to a discussion on SAR was quite interesting.\nThe rice crop monitoring case study is quite interesting, I did not delve into too deeply this week (purely through lack of time), but it is something I will explore further. I am planning to conduct my dissertation on mapping the wine industry of the UK, wherein identifying nuanced characteristics, which do not seem possible given the immense distances between the sensor and sensed object.\nSAR strikes me as a very powerful sensor and indeed its ability to be operational at night as well as during the day (largely, unlike other sensors)."
  },
  {
    "objectID": "Week4.html#summary",
    "href": "Week4.html#summary",
    "title": "4  Policy",
    "section": "4.1 Summary",
    "text": "4.1 Summary\nJakarta faces a significant risk of flooding due to a combination of geographical, environmental, and human factors (1) (2):\n\nGeographical Location: Jakarta is situated on the northwest coast of Java, at the mouth of the Ciliwung River on Jakarta Bay, which leads to the Java Sea. This coastal and low-lying geography makes it inherently vulnerable to flooding from the sea, especially during high tide events.\nGround Water Extraction: Excessive groundwater extraction has caused Jakarta to sink at an alarming rate, with some areas experiencing subsidence rates of more than 10 cm per year. This subsidence exacerbates the risk of flooding, as the city becomes increasingly lower than sea level.\nUrbanisation and Loss of Green Spaces: Rapid urban development has led to the loss of green spaces and water-absorbent areas, increasing runoff and reducing the natural absorption of rainwater. This situation is worsened by the fact that many developments occur without proper consideration for their environmental impact, contributing to the city’s vulnerability to floods.\nClimate Change: The effects of climate change, including rising sea levels and more intense rainfall events, heighten the risk of both coastal and pluvial flooding in Jakarta. These relate to many of the reasons mentioned here.\nInadequate Infrastructure: Jakarta’s drainage and flood management infrastructure are often inadequate to handle the volume of water from heavy rains and high tides, partly due to the rapid pace of urban development and challenges in governance and maintenance.\nRiver Overflow: The city is crossed by 13 rivers, and the overflow from these rivers during heavy rains is a significant cause of flooding in many parts of Jakarta.\n\nThese factors, create a complex challenge for flood risk management in Jakarta, especially due to the high degree of overlap and interconnectedness of these issues. Addressing these problems requires a nuanced approach that includes improving infrastructure (basic infrastructure, and climate change adaption infrastructure), regulating groundwater extraction, enhancing urban planning to forefront sustainability issues, and further action to mitigate the effects of climate change.\n\n4.1.1 Jakarta’s Current Strategy\nJakarta’s strategy for managing its significant flood risk has evolved to include both infrastructural developments and nature-based solutions (NbS), addressing the city’s unique challenges such as extreme urbanization, land subsidence, and the impacts of climate change.\nThe Jakarta Coastal Defense Strategy (JCDS) and the subsequent National Capital Integrated Coastal Development Masterplan (NCICD), also known as the “Giant Sea Wall” or “Great Garuda” project, stand out as major infrastructural components of Jakarta’s flood management efforts. Initiated with Dutch collaboration, these projects are aiming to protect the city from sea-level rise and coastal flooding by constructing large sea walls and creating retention areas to manage excess water. This strategy is being complemented by efforts to regulate river flows and expand flood reservoirs, as well as the clearing of waterways to restore their capacity. However, the Giant Sea Wall has come under criticism with Garschagen et al (2018) describing is as “[cemeting] rather than [transforming] the current risk management paradigm which gravitates around the goal of controlling flood symptoms, rather than addressing their largely anthropogenic root causes.”\n\n\n\n\n\nThe Great Garuda Project - Masterplan\n\n\n\n\n\n\nMasterplan Render\n\n\n\n\nDespite the ambitious plans and potential benefits of these strategies, Jakarta faces challenges such as budgetary constraints, poor maintenance of existing infrastructure, and the need for significant resettlement to accommodate new projects. Additonally, the success of these initiatives requires overcoming bureaucratic and corruption-related hurdles that have historically impeded large-scale urban projects in Indonesia."
  },
  {
    "objectID": "Week4.html#application",
    "href": "Week4.html#application",
    "title": "4  Policy",
    "section": "4.2 Application",
    "text": "4.2 Application\nGarschagen et al (2018) identify that Jakarta is not being held back by lack of will for change - quite the opposite. They describe that despite the scale of the planned projects (such as the Great Garuda project), solutions have been found to many, typical, issues including financing or technical barriers. Instead the main challenge is within governance and related to issues within “societal and political negotiation of conflicting visions and paradigms for risk reduction, and especially the burden-sharing of negative side-effects” (ibid, pp. 15).\nConsequently, a move which may be highly beneficial for Jakarta is more transparent data sharing and discussion. The development of an online quasi dashboard forum.\n\nIn Barcelona, Decidim is a digital platform for citizen participation - it allows the urban realm to be discussed with participation at the forefront. Citizens are effectively invited to design and improve upon the participatory process; invited to contribute proposals that will be debated (which could translate into binding legislation, provided certain thresholds are met); and invited to monitor and assess both the process in its procedures as in its outcomes.\n\n\nIn Singapore, Virtual Singapore, a dynamic digital twin of the city-state, simulates the urban environment through integrates intricate details of the city along with real-time dynamic data, facilitating a range of urban planning and management tasks. It uses an extensive range of data sources including air quality, temperature, and noise. It allows different scenarios to be tested; for emergency preparedness, urban planning considerations, development collaboration, environmental impacts, among others. Whilst it is aimed at government and development organisations, it will be made accesible to the public in the future.\n\nJakarta could utilise these case studies. Through the use of rain gauges, historic weather data, water level sensors, satellite imagery and population data, Jakarta could build an online dashboard utilising the most up-to-date information.\nEsri Indonesia’s ESSC has developed a flood map portal, however it does not go far enough and could benefit from more dynamic satellite and population data.\nSatellite imagery could be attained from Sentinel-1 and Sentinel-2, due to their ability to provide global coverage to high spatial and temporal resolutions. Sentinel-1 is equipped with a synthetic aperture radar (SAR), and so can penetrate clouds and provide imagery during day and night, making it ideal for rapid mapping of flood events and supporting early warning systems. Sentinel-2 complements this with high-resolution optical imaging, useful for monitoring inundation events and assessing damage post-crisis, although its effectiveness can be hampered by cloud cover during rainfall events.\nPopulation data can be synthesised, along with ‘on the ground’ sensors to assess, both in real-time and through forecasting, locations at the greatest risk, taking into account impacted population size, likely infrastructural damage, and scale of impact.\nThrough making this accessible to citizens, they are able to prepare in the event of an emergency situation, but are also able to reconcile their own experiences of flooding with an quantifiable collective understanding of the risks it poses. This could allow more informed discussions where citizens and the government engage openly.\n\nFlood Mapping and Monitoring: SAR data allows for the real-time mapping of flood extents, even under cloud cover, which is crucial for timely evacuation and response.\nRisk Assessment: Combining SAR data with DEMs and population data helps identify flood-prone areas, critical for planning and implementing flood defenses.\nInfrastructure Planning: Insights from remotely sensed data support the design and placement of flood mitigation infrastructure, like dams and improved drainage systems.\nDisaster Preparedness and Response: The data enhances the efficiency of disaster response efforts by identifying affected areas and prioritizing rescue and relief operations.\n\nLink to Global Goals\nJakarta’s efforts to manage flooding through remotely sensed data contribute to the Sustainable Development Goals (SDGs), particularly:\n\nSDG 11 (Sustainable Cities and Communities): By enhancing urban resilience against flooding.\nSDG 13 (Climate Action): Contributing to disaster risk reduction in the face of climate change."
  },
  {
    "objectID": "Week4.html#reflection",
    "href": "Week4.html#reflection",
    "title": "4  Policy",
    "section": "4.3 Reflection",
    "text": "4.3 Reflection\nI particularly enjoyed this week as it has lent itself a lot to what I studied for my BSc - urban planning. The case of Jakarta is severe - it is one of the largest megacities in the world and is facing an immense crisis regarding flooding. Its sinking rate of 10cm a year is quite phenomenal - that is one meter every 10 years (so, in theory, has sunk over 2m over the course of my life, which is an astounding thought). [Edit from week 9: SAR could be implemented in this regard to the proposed dashboard for its ability to detect elevations and understand where the city is sinking at the most rapid rates].\nThese solutions are not easy - whilst it is very easy to say that they ‘simply need to develop a dashbaord’, the issue goes beyond that. Responses to urban issues are often complex, with many competing interests and desires. Further, the inevitable issue of project financing, especially in a country where corruption is an issue and access to capital, whilst not critically lacking, leaves more to be desired for an optimal response. When looking at policy documents and urban issues, it is easy to become overwhelmed by the sheer volume of policy documentation (which is often bulky, not completely up-to-date, and hard to naviagate) and resident desires and needs.\nAdditionally, there can often be issues with citizen participation in planning issues. This is especially the case in the UK (as in many other countries), a context I am accutely aware of from experience. Local elections typically achieve 30% turnout rates, and local planning issues typically can often receive far less attention than that. My initial assumption was that RS technology would be able to more effectively engage people through its ability to provide ‘objective’ data at a global level. However in all data issues - the insights are highly dependent on the individual that produces that insight (through bias, selective use of data, etc.), but also due the relative complexity of RS technology, it may scare many people. However, a recent Guardian article, Britons go map-crazy, with geographical games and books becoming bestsellers, demonstrates that we do have an interest in mapping and that maps are a great way to engage the public. By also adopting RS insights into maps, governments may be able to more effectively engage citizens. Of course this is reliant on governmental bodies providing good governance and leadership."
  }
]